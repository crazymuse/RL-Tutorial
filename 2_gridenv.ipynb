{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridEnv\n",
    "In this example, we will be looking at making use of Grid Env to understand the concepts of Reinforcement Lerning. The model (\"gridEnv\") gives us an easy way to display and interact with grid, where C indicates the agent's position and T stands for Rewarding Terminal state(+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gridEnv import Grid;\n",
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function\n",
    "* Hit the walls : -1\n",
    "* Hit the Terminal state : +1 (End of the Episode)\n",
    "* Rest of the times : +0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|. . . . . .|\n",
      "|. . . . . .|\n",
      "|. . . . . .|\n",
      "|C . . . . T|\n",
      "(False, 0, [2, 0])\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep;\n",
    "g = Grid(length=4,width=6,start=(0,0),terminals=[(3,5)]);\n",
    "for i in range(10):\n",
    "    g.display()\n",
    "    print (g.step(3))\n",
    "    sleep(0.5);\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Episode\n",
    "We are just creating a simple loop untill the C reaches T. \n",
    "## Terms\n",
    "* State : index of C\n",
    "* Action : North, East, West, South (NEWS)\n",
    "* Reward on action : +1,-1,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|C . . . . .|\n",
      "|. . . . . .|\n",
      "|. . . . . .|\n",
      "|. . . . . T|\n",
      "Loop Ended\n"
     ]
    }
   ],
   "source": [
    "hasEnded = False;\n",
    "g.reset()\n",
    "while(hasEnded == False):\n",
    "    g.display()\n",
    "    print (g.step(0,update=False),g.step(1,update=False),g.step(2,update=False),g.step(3,update=False))\n",
    "    hasEnded,reward,nextpos = g.step(g.getbestaction(),epsilon=0.7)\n",
    "    sleep(0.05);\n",
    "    clear_output(wait=True)\n",
    "g.display()\n",
    "print ('Loop Ended')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|. . . C . .|\n",
      "|. . . . . .|\n",
      "|. . . . . .|\n",
      "|. . . . . T|\n",
      "[[0.21 0.26 0.33 0.41 0.51 0.64]\n",
      " [0.26 0.33 0.41 0.51 0.64 0.8 ]\n",
      " [0.03 0.41 0.05 0.04 0.8  1.  ]\n",
      " [0.04 0.51 0.64 0.8  1.   0.  ]]\n",
      "|→ → → → → ↓|\n",
      "|→ → → → → ↓|\n",
      "|↑ ↓ ← ↑ → ↓|\n",
      "|↑ → → → → ←|\n"
     ]
    }
   ],
   "source": [
    "gamma =0.8;\n",
    "for i in range(50):\n",
    "    g.display()\n",
    "    print (g.value);\n",
    "    rary=[]\n",
    "    for a in range(4):\n",
    "        hasEnded,reward,nextpos = g.step(a,epsilon=1,update=False)\n",
    "        vnext = g.value[nextpos[0],nextpos[1]]\n",
    "        rary.append(reward+gamma*vnext)\n",
    "    bestaction,bestdiscreward = np.argmax(rary),np.max(rary)\n",
    "    curpos=list(g.curpos)\n",
    "    g.step(g.policy[g.curpos[0],g.curpos[1]],epsilon=0.9);\n",
    "    g.policy[curpos[0],curpos[1]]=bestaction;\n",
    "    g.value[curpos[0],curpos[1]] = bestdiscreward;\n",
    "    g.displayPolicy()\n",
    "    sleep(0.05);\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned Policy\n",
    "It displays action with highest q value for a given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|→ → → → → ↓|\n",
      "|→ → → → → ↓|\n",
      "|↑ ↓ ← ↑ → ↓|\n",
      "|↑ → → → → ←|\n"
     ]
    }
   ],
   "source": [
    "g.displayPolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learned State Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.21 0.26 0.33 0.41 0.51 0.64]\n",
      " [0.26 0.33 0.41 0.51 0.64 0.8 ]\n",
      " [0.03 0.41 0.05 0.04 0.8  1.  ]\n",
      " [0.04 0.51 0.64 0.8  1.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "print (g.value);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
